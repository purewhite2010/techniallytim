<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Technically Tim</title><link href="http://tim.purewhite.id.au/" rel="alternate"></link><link href="http://tim.purewhite.id.au/feeds/blog-technical.atom.xml" rel="self"></link><id>http://tim.purewhite.id.au/</id><updated>2011-11-05T12:10:00+10:00</updated><entry><title>Belkin? Rtkit?</title><link href="http://tim.purewhite.id.au/2011/11/belkin-rtkit/" rel="alternate"></link><updated>2011-11-05T12:10:00+10:00</updated><author><name>Tim White</name></author><id>tag:tim.purewhite.id.au,2011-11-05:2011/11/belkin-rtkit/</id><summary type="html">&lt;p&gt;While attempting to remotely debug a linux machine today, I was first
encountering a strange problem. Any process that took more than about
1/2 second to complete, would freeze. top, ps, lsmod, tail -f, the list
goes on. For example, trying to run dmesg and display it's output would
freeze, but dmesg into a file, and it would complete!&lt;/p&gt;
&lt;p&gt;After much digging, I eventually found that rtkit (rtkit-daemon) is
constantly trying to make pulseaudio operate at realtime. In reality, we
don't need our audio to operate in realtime as most modern computers can
keep up with video playback just fine. For the few people we actually
want near real time audio (say, people recording multitrack stuff), then
they can enable it themselves. Disabling rtkit (actually, uninstalling
it as it appears to be started in dbus stuff), seems to have solved that
problem.&lt;/p&gt;
&lt;p&gt;The next problem was a strange DNS response. A dns request through the
Belkin modem, to this server (purewhite.id.au) would return 10.45.41.175
instead of 175.41.45.10. I know what reverse DNS is, but this is reverse
IP! Belkin is returning the ip in reverse!! (Or backwards if you
desire). A quick check reveals that this relatively new modem, hasn't
got any new firmware for it (and it's firmware is over 1 year old).
Apparently, someone else had this problem and belkin told them to just
hard code the ip's in your hosts file for the hosts that are being
returned wrong! I believe it was also a Belkin modem that would return
strange results when you did an AAAA request (ipv6).&lt;br /&gt;
So if you have a Belkin, maybe force your computer to use your ISP's
DNS servers directly, rather than the routers. (Or take it back to the
shop, because after all, it is faulty)&lt;/p&gt;</summary><category term="backwards ip"></category><category term="belkin"></category><category term="reverse ip"></category><category term="rtkit"></category></entry><entry><title>Nginx, PHP-FPM, Wordpress, Super Cache</title><link href="http://tim.purewhite.id.au/2011/10/nginx-php-fpm-wordpress-super-cache/" rel="alternate"></link><updated>2011-10-24T11:37:00+10:00</updated><author><name>Tim White</name></author><id>tag:tim.purewhite.id.au,2011-10-24:2011/10/nginx-php-fpm-wordpress-super-cache/</id><summary type="html">&lt;p&gt;So recently I've been exploring the alternative world of Nginx instead
of Apache, and PHP-FPM instead of mod_php. There are plenty of
tutorials on the net for getting all of this setup, however not that
many are up to date anymore for the Super Cache stuff. Hopefully what I
present here will be a more up to date config, that is also mostly
secure compare to a good number of ones on the net (to do with passing
non PHP files to the php interpreter).&lt;/p&gt;
&lt;p&gt;Firstly, my Nginx config for this very blog.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;server&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kn"&gt;server_name&lt;/span&gt; &lt;span class="s"&gt;www.tim.purewhite.id.au&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="kn"&gt;rewrite&lt;/span&gt; &lt;span class="s"&gt;^/(.*)&lt;/span&gt; &lt;span class="s"&gt;http://tim.purewhite.id.au/&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt; &lt;span class="s"&gt;permanent&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;server&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kn"&gt;server_name&lt;/span&gt; &lt;span class="s"&gt;tim.purewhite.id.au&lt;/span&gt; &lt;span class="s"&gt;static.tim.purewhite.id.au&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kn"&gt;root&lt;/span&gt; &lt;span class="s"&gt;/home/tim/domains/tim.purewhite.id.au/public_html&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="kn"&gt;access_log&lt;/span&gt; &lt;span class="s"&gt;/var/log/nginx/tim.purewhite.id.au_access_log&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kn"&gt;access_log&lt;/span&gt;  &lt;span class="s"&gt;/var/log/nginx/default.access.log&lt;/span&gt; &lt;span class="s"&gt;host_combined&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="c1"&gt;#access_log  /var/log/nginx/uri.log host_combined_uri;&lt;/span&gt;
    &lt;span class="kn"&gt;error_log&lt;/span&gt; &lt;span class="s"&gt;/var/log/nginx/tim.purewhite.id.au_error_log&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="kn"&gt;index&lt;/span&gt; &lt;span class="s"&gt;index.php&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="kn"&gt;location&lt;/span&gt; &lt;span class="s"&gt;/&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

        &lt;span class="kn"&gt;if&lt;/span&gt; &lt;span class="s"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$http_cookie&lt;/span&gt; &lt;span class="p"&gt;~&lt;/span&gt; &lt;span class="sr"&gt;&amp;quot;comment_author_|wordpress|wp-postpass_&amp;quot;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="kn"&gt;rewrite&lt;/span&gt; &lt;span class="s"&gt;^/(.*)&lt;/span&gt; &lt;span class="s"&gt;/loggedin&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt; &lt;span class="s"&gt;last&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="kn"&gt;try_files&lt;/span&gt; &lt;span class="nv"&gt;$uri&lt;/span&gt;
        &lt;span class="s"&gt;/wordpress/wp-content/cache/supercache/&lt;/span&gt;&lt;span class="nv"&gt;$http_host/$uri/index.html&lt;/span&gt;
        &lt;span class="nv"&gt;$uri/&lt;/span&gt;
        &lt;span class="s"&gt;/index.php&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="kn"&gt;location&lt;/span&gt; &lt;span class="s"&gt;/loggedin&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="kn"&gt;internal&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kn"&gt;rewrite&lt;/span&gt; &lt;span class="s"&gt;^/loggedin(.*)&lt;/span&gt; &lt;span class="s"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt; &lt;span class="s"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kn"&gt;try_files&lt;/span&gt; &lt;span class="nv"&gt;$uri&lt;/span&gt; &lt;span class="nv"&gt;$uri/&lt;/span&gt; &lt;span class="s"&gt;/index.php&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;


    &lt;span class="kn"&gt;location&lt;/span&gt; &lt;span class="s"&gt;^~&lt;/span&gt; &lt;span class="s"&gt;/code&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="kn"&gt;proxy_set_header&lt;/span&gt; &lt;span class="s"&gt;Host&lt;/span&gt; &lt;span class="nv"&gt;$host&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="kn"&gt;proxy_set_header&lt;/span&gt; &lt;span class="s"&gt;X-Forwarded-Server&lt;/span&gt; &lt;span class="nv"&gt;$host&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="kn"&gt;proxy_set_header&lt;/span&gt; &lt;span class="s"&gt;X-Forwarded-Host&lt;/span&gt; &lt;span class="nv"&gt;$host&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="kn"&gt;proxy_set_header&lt;/span&gt; &lt;span class="s"&gt;X-Forwarded-For&lt;/span&gt; &lt;span class="nv"&gt;$proxy_add_x_forwarded_for&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kn"&gt;proxy_pass&lt;/span&gt; &lt;span class="s"&gt;http://127.0.0.1:8080/code/&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="kn"&gt;location&lt;/span&gt; &lt;span class="p"&gt;~&lt;/span&gt;&lt;span class="sr"&gt;*&lt;/span&gt; &lt;span class="s"&gt;\.(ico|css|js|gif|jpe?g|png)&lt;/span&gt;$ &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="kn"&gt;expires&lt;/span&gt; &lt;span class="s"&gt;1w&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kn"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;



    &lt;span class="kn"&gt;fastcgi_intercept_errors&lt;/span&gt; &lt;span class="no"&gt;off&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="kn"&gt;location&lt;/span&gt; &lt;span class="p"&gt;~&lt;/span&gt; &lt;span class="sr"&gt;\.php&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="kn"&gt;try_files&lt;/span&gt; &lt;span class="nv"&gt;$uri&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;404&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kn"&gt;include&lt;/span&gt; &lt;span class="s"&gt;fastcgi_params&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kn"&gt;fastcgi_pass&lt;/span&gt;   &lt;span class="n"&gt;127.0.0.1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;9002&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="kn"&gt;include&lt;/span&gt; &lt;span class="s"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first thing to notice is line 1-4. This simply redirects everyone
from www.tim.purewhite.id.au to tim.purewhite.id.au. Simple as that.&lt;/p&gt;
&lt;p&gt;Next we define the server and the root document path. Still very
standard. Then we define access logs, for various reason I'm logging to
more than one place, but that'll change once everything is finished.&lt;/p&gt;
&lt;p&gt;Line 15 is boring, we just define the "index index.php" so that if you
access a directory it will load index.php or give you a 404 (which it
won't because of things further down).&lt;/p&gt;
&lt;p&gt;Now for the fun. Lines 19-21. These catch a logged in user and send them
on an internal redirect down to lines 28-32. This is so we don't serve
cached content to logged in users. That little snippit is thanks to a
post at &lt;a href="http://permalink.gmane.org/gmane.comp.web.nginx.english/15664"&gt;http://permalink.gmane.org/gmane.comp.web.nginx.english/15664&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, there was a problem in the rest of the code. Thanks to a post
at
&lt;a href="http://wordpress.org/support/topic/lack-of-nginx-support-from-wp-super-cache"&gt;http://wordpress.org/support/topic/lack-of-nginx-support-from-wp-super-cache&lt;/a&gt;
I realised we needed to test if the cache was being used or not. So I
added the extra logging and discovered it wasn't. I quickly worked out
what the problem was. The code at lines 22 - 25 had the middle 2 lines
swapped around. So "\$uri/" was before the supercache line. What this
mean was that it would try if the \$uri was a directory, and to load a
directory it would try index.php (due to the index line) and so would
end up loading wordpress through index.php. However, if we try the
supercache line first, we find the cache file and so don't need to load
indexes.&lt;/p&gt;
&lt;p&gt;And just like that, magic, it works! We use supercache files for normal
users, and if a cache file doesn't exist, we load wordpress like normal!&lt;/p&gt;
&lt;p&gt;I'm also looking at how we run Nginx and PHP-FPM. I have heard of a few
ways, one being that root runs a Nginx as user nginx or nobody, and each
user runs their own Nginx which we proxy to from the main one. (And
users run their own PHP-FPM as well). This sounds like a lot of work,
very complicated, but yes, it gives you absolute security as only the
user can access his web docs and scripts, and everything runs as that
user. No one else's php process can load your config file to discover
your database passwords.&lt;/p&gt;
&lt;p&gt;Another way of running it is with Nginx as a nginx/nobody/www-data user,
and each user run their own php-fpm but give the nginx/nobody/www-data
user read only access to the web directory. If done correctly, this can
actually be very secure. First, (as root) you chgrp all the files and
directories in the users doc root (htdocs, www, public_html etc) to the
user nginx will run as. Ideally, you also only allow them read access
(so `chmod g+rX,g-w -R public_html` will give them access to read,
but not write). You then set the gid bit on the directory; `chmod g+s
public_html` (and do this for any directories that already exist
underneath). Now any files the user creates underneath the public_html
dir will be readable to the nginx user, so nginx can serve static files.
Now running php-fpm as each user (I use php-fpm with a pool per user),
the php process can read all the files that user can, so only the users
own php process can read their config files with the password in it! And
it also means that files you upload (i.e. wordpress media files) will be
owned by the user, not by www-data or what ever the web user is. This is
SO much better than Apache and mod_php, and easier than suExec with
mod_php.&lt;/p&gt;
&lt;p&gt;Once I have more of my domains moved to Nginx, I'll do a report on
memory and cpu usage.&lt;/p&gt;</summary><category term="Nginx"></category><category term="php-fpm"></category><category term="secure"></category><category term="supercache"></category><category term="try_files"></category><category term="wordpress"></category></entry><entry><title>Munin cgi graph timing out</title><link href="http://tim.purewhite.id.au/2011/09/munin-cgi-graph-timing-out/" rel="alternate"></link><updated>2011-09-01T11:50:00+10:00</updated><author><name>Tim White</name></author><id>tag:tim.purewhite.id.au,2011-09-01:2011/09/munin-cgi-graph-timing-out/</id><summary type="html">&lt;p&gt;A problem that has given lots of people problems, caused me issues
yesterday. I have munin using munin-cgi-graph to create the graphs on
demand due to me not often viewing the graphs. A few days ago I had a
server issue that caused apache to lock up (I think a process ran away
with my RAM which caused swapping and apache to lock up.) Once I apache
running again, I wanted to check the munin graphs to see what the system
looked like during the lockup (which killed a number of processes due to
out of memory conditions). However, the graphs wouldn't generate and the
cgi was timing out without sending any data.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Timeout waiting for output from CGI script /usr/lib/cgi-bin/munin-cgi-graph Premature end of script headers: munin-cgi-graph&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;I'm not alone ether.
&lt;a href="http://wiki.kartbuilding.net/index.php/Further_issues_upgrading_to_Lenny#munin_with_cgi"&gt;http://wiki.kartbuilding.net/index.php/Further_issues_upgrading_to_Lenny#munin_with_cgi&lt;/a&gt;
and &lt;a href="http://forum.linode.com/viewtopic.php?t=5171%3E"&gt;http://forum.linode.com/viewtopic.php?t=5171%3E&lt;/a&gt; both had issues.
More googling still didn't find an answer so I tried to debug the perl
cgi. After using CPAN to get Devel:Trace installed, I discovered the cgi
was sitting waiting for a semaphore flag that it uses to ensure no more
than a certain number of munin-graphs are running at once. This is
great, except when a crash has caused this semaphore to be stuck at the
maximum so no more munin-graph processes get started ever!&lt;/p&gt;
&lt;p&gt;There are 2 solutions. The first is simple, reboot. The second is also
simple, clear the semaphore flags manually. &lt;code&gt;ipcs&lt;/code&gt; is the command to
show the flags and &lt;code&gt;ipcrm&lt;/code&gt; is the command for removing the semaphores.
Check the man pages for information on the correct syntax.&lt;/p&gt;</summary><category term="ipcrm"></category><category term="ipcs"></category><category term="munin"></category><category term="munin-cgi-graph"></category><category term="semaphore"></category><category term="timeout"></category></entry><entry><title>Gmail account hacked :(</title><link href="http://tim.purewhite.id.au/2011/08/gmail-account-hacked/" rel="alternate"></link><updated>2011-08-04T11:28:00+10:00</updated><author><name>Tim White</name></author><id>tag:tim.purewhite.id.au,2011-08-04:2011/08/gmail-account-hacked/</id><summary type="html">&lt;p&gt;For someone that prides himself on security, it is rather embarrassing
to get hacked. I currently don't know how they got in, or exactly what
they have done. I have changed my passwords and security questions
though.&lt;/p&gt;
&lt;p&gt;So far, I know that my account was accessed from Poland (194.181.62.13)
with last access at 8:49 am today. It only appears to have been accessed
via a browser, and there don't seem to be any extra filters (forwarding)
setup. I do know that it appears everyone in my address book has been
sent a link, although the link I can see that was sent (thanks to a
bounce back) doesn't appear to work. They also deleted everything from
my sent box and trash, which is probably what annoys me the most. I
don't know if anything else has been deleted.&lt;/p&gt;
&lt;p&gt;The most likely method they got in by was from an attack on another
site, that revealed my password from when I used the same password in
more than one place. As I no longer do this, I've been slowly changing
my passwords to all be unique, however I should have changed my gmail
one along time ago.&lt;/p&gt;
&lt;p&gt;So 2 morals to this story. Even security conscience people can get
hacked. And backup your data from the cloud if you wish to avoid loosing
anything. I'm now in the process of setting up version controlled
backups of my gmail data.&lt;/p&gt;</summary><category term="gmail"></category><category term="hacked"></category></entry><entry><title>Mt Gox Passwords Leaked</title><link href="http://tim.purewhite.id.au/2011/06/mt-gox-passwords-leaked/" rel="alternate"></link><updated>2011-06-20T14:37:00+10:00</updated><author><name>Tim White</name></author><id>tag:tim.purewhite.id.au,2011-06-20:2011/06/mt-gox-passwords-leaked/</id><summary type="html">&lt;p&gt;For the second time in a week, I've heard of a websites user database
being leaked. In the first case it was from a site I've never used. The
second though was a site I signed up to a few months back.&lt;br /&gt;
One of the biggest problems with this leaked database is that the
hashing function used isn't that strong when the hacker has rainbow
tables to use to crack the database.&lt;br /&gt;
The first side effect of this for me was to go and change some of my
passwords as a precautionary measure. The second side effect, and the
more annoying one, is that I used a private email address for this
particular account instead of using one of my "junk" gmail addresses. So
now my private email address is in the hands of every hacker who is
trying to crack that database. And already we are receiving "spam" to
those addresses in that database. Most of it so far is users ether
letting you know the Mt Gox database has been hacked, or users/owners of
other Bitcoin exchanges sending you "advertising" so you'll come start
using their exchange. I've email gotten an email advertising online
storage from a company that accepts Bitcoins as payments. And they
haven't bothered to try and keep the email addresses slightly private,
1500 other people also have my address, and I have theirs, as no Bcc was
used. (Of course, spam filtering will quickly filter that particular
email out).&lt;br /&gt;
Interested to see how bad the compromise was, and if it'll affect me,
I've also downloaded the user database now. A quick look shows that my
password is hashed with the less secure method and a quick bit of code
later I can confirm the password I used to make that hash. Luckily for
me, I use pwdhash to generate a unique password for each site I use.
This means that an attacker who has cracked my hashed password in the Mt
Gox password, still only has a password that can be used for one site,
Mt Gox. If they had enough time and power, then maybe they could work
backwards and eventually find the password I used to generate my pwdhash
passwords, but by the time they did this, I'd have changed all those
passwords anyway.&lt;br /&gt;
Having only been using pwdhashing for a little while now, it was good
to discover that it has already protected me from an attack. A number of
user who had simple passwords that have been cracked already, have also
had other accounts attacked as they used the same password in multiple
places.&lt;/p&gt;
&lt;p&gt;An interesting side note is how much the Mt Gox Bitcoin exchange has
grown in a very short space of time. A discussion taking place in a
forum noted that your position in the database is related to when you
signed up. Working from knowing when you signed up shows how many people
signed up after you. It seems to have had exponential growth in the last
few weeks, which is good for Bitcoin in general, but bad once you
realise how this will look to all those new users. Looking at my
position in the database, I can see I was a very early adopter.&lt;/p&gt;</summary><category term="bitcoin"></category><category term="crack"></category><category term="database"></category><category term="leaked"></category><category term="mt gox"></category><category term="password"></category></entry><entry><title>Drupal Upgrades</title><link href="http://tim.purewhite.id.au/2011/06/drupal-upgrades/" rel="alternate"></link><updated>2011-06-10T16:42:00+10:00</updated><author><name>Tim White</name></author><id>tag:tim.purewhite.id.au,2011-06-10:2011/06/drupal-upgrades/</id><summary type="html">&lt;p&gt;Drupal upgrades have not been easy, and they should be easy. You look at
wordpress, one click and the upgrade is done! Plugins, themes and core
can all be upgraded from in the browser.&lt;/p&gt;
&lt;p&gt;Drupal upgrades are traditionally backup, move all files out, extract
new fresh files, move selected files back. Now there is an easier way!
Patch files from &lt;a href="http://fuerstnet.de/en/drupal-upgrade-easier"&gt;http://fuerstnet.de/en/drupal-upgrade-easier&lt;/a&gt; allow
you to backup, and then inplace upgrade the Drupal core files! No nasty
moving things around that is almost guaranteed to break something
because you forgot to move something back.&lt;/p&gt;
&lt;p&gt;I still look forward to the day when Drupal upgrades are as easy as
Wordpress. Until then, I have a good enough method!&lt;/p&gt;</summary><category term="drupal"></category><category term="upgrades"></category></entry><entry><title>Disable IPv6 in Transmission BT</title><link href="http://tim.purewhite.id.au/2011/06/disable-ipv6-in-transmission-bt/" rel="alternate"></link><updated>2011-06-10T15:59:00+10:00</updated><author><name>Tim White</name></author><id>tag:tim.purewhite.id.au,2011-06-10:2011/06/disable-ipv6-in-transmission-bt/</id><summary type="html">&lt;p&gt;After finally getting my IPv6 working nicely, it was time to prevent
Transmission from using IPv6 as  I don't want lots of torrent traffic
going through the tunnel when it's faster through IPv4 (until a time I
can get Native IPv6). Apparently this is an "invalid" feature request
according to some of the developers.
(&lt;a href="http://trac.transmissionbt.com/ticket/4197"&gt;http://trac.transmissionbt.com/ticket/4197&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Had the developer actually stopped to consider it, maybe read some
relevant parts of the source code, they would have quickly discovered
that you can already disable it! They could document it as a feature
without having to touch a line of code, and mark the feature request as
completed!&lt;/p&gt;
&lt;p&gt;It's a rather simple fix. There are checks for the IPv6 address not
being a link local address, or a 6to4, or Teredo tunnel[1]. So we just
make Transmission bind to a link local address and hey presto, no IPv6
for Transmission!&lt;/p&gt;
&lt;p&gt;Simply add the following line to the settings.json file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;quot;bind-address-ipv6&amp;quot;: &amp;quot;fe80::&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;[1] You'd think given that they already prevent Teredo tunnels from
being used, that the feature request would actually make sense for those
wishing to disable IPv6 due to TUNNEL's!&lt;/p&gt;</summary><category term="ipv6"></category><category term="torrent"></category><category term="transmission"></category><category term="tunnel"></category></entry><entry><title>Why we need native IPv6</title><link href="http://tim.purewhite.id.au/2011/06/why-we-need-native-ipv6/" rel="alternate"></link><updated>2011-06-10T12:12:00+10:00</updated><author><name>Tim White</name></author><id>tag:tim.purewhite.id.au,2011-06-10:2011/06/why-we-need-native-ipv6/</id><summary type="html">&lt;p&gt;We need native IPv6, or at least a decent PoP in Australia!&lt;/p&gt;
&lt;p&gt;Currently our home network is IPv6 enabled via a Sixxs tunnel. If we
lived in NZ then our PoP would be in NZ. Unfortunately we can't use the
NZ PoP, so instead we use the London PoP! Eventually I'll get around to
pinging every PoP available to us and find the "closest" one, but for
now, letter the numbers do the talking.&lt;/p&gt;
&lt;p&gt;I ping the same machine both via IPv6 and via IPv4. Lets see if you can
work out which is which.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;10 packets transmitted, 10 received, 0% packet loss
rtt min/avg/max/mdev = 698.592/712.159/814.473/34.163 ms

10 packets transmitted, 10 received, 0% packet loss
rtt min/avg/max/mdev = 76.670/79.557/87.452/2.866 ms
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The PoP has an average ping of 350ms just to get to the PoP! No wonder
it takes so long to get to the PoP and back to Australia! Hopefully
later in the year my hosting provider will have fixed the IPv6 transport
and I can setup my own local tunnel. Until then, slow IPv6 :(&lt;/p&gt;
&lt;p&gt;&lt;ins datetime="2011-06-10T02:48:46+00:00"&gt;Edit: So I finally got AARNet
IPv6 tunnel broker service working. A much better improvement. I'm
running both tunnels in parrallel so that if one dies the other is
working. Hopefully I'll see better IPv6 improvement now. Still, native
IPv6 would be better.
&lt;a href="http://michael-wheeler.org/2009/03/24/australian-ipv6-tunnel-broker/"&gt;http://michael-wheeler.org/2009/03/24/australian-ipv6-tunnel-broker/&lt;/a&gt;&lt;/ins&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;10 packets transmitted, 10 received, 0% packet loss
rtt min/avg/max/mdev = 235.587/267.187/382.010/45.090 ms
&lt;/pre&gt;&lt;/div&gt;</summary><category term="ipv6"></category><category term="ping"></category><category term="sixxs"></category></entry><entry><title>Disk recovery - Which files are damaged?</title><link href="http://tim.purewhite.id.au/2011/04/disk-recovery-which-files-are-damaged/" rel="alternate"></link><updated>2011-04-19T09:40:00+10:00</updated><author><name>Tim White</name></author><id>tag:tim.purewhite.id.au,2011-04-19:2011/04/disk-recovery-which-files-are-damaged/</id><summary type="html">&lt;p&gt;&lt;ins datetime="2011-08-11T21:49:04+00:00"&gt;First, getting an image of the
damaged hard drive.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;ddrescue -n /dev/inputdevice rescued.img rescued.log

&lt;span class="nv"&gt;$ &lt;/span&gt;ddrescue -r &lt;span class="m"&gt;1&lt;/span&gt; /dev/inputdevice rescued.img rescued.log
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first ddrescue command tries to fly through the disk as quickly as
possible, skipping over sections when an error occurs. This allows me to
recover most of the good data as quickly as possible. The second command
(which is only needed if you had errors found with the first command)
will then retry the bad sections of disk, splitting the error sections
into smaller and smaller parts until you eventually have individual
blocks that are damaged. It is here that ddrescue really works hard to
recover your data, and this part can take just as long as the first
part.&lt;/ins&gt;&lt;/p&gt;
&lt;p&gt;So you have successfully used ddrescue to recover everything you can off
a failing hard drive. Now you have a big image file, maybe 500Gb, with
sectors that could be recovered, and those that couldn't. But how do you
find which files belong to the broken sectors? I recently had this
problem with a FAT32 filesystem. After lots and lots of googling, I
still didn't have a decent answer. Windows has a tool called DiskView,
however it doesn't appear to work on disk images. There is also a Hex
viewer around that apparently will tell you which files belong to the
sector you are viewing, but I had no luck with that ether.&lt;/p&gt;
&lt;p&gt;Eventually I stumbled across a toolset I should have used from the
start. The Sleuth Kit. I also stumbled across a paper someone had
written doing some forensics with The Sleuth Kit which pointed me to the
right tools, although some had changed names.&lt;/p&gt;
&lt;p&gt;However, let me first point you to a fairly simple way that is hidden in
the ddrescue info pages. While at first it sounds like this method
should be the long hard way, it actually works out to be the easiest
way.&lt;/p&gt;
&lt;p&gt;It is suggested in the ddrescue info pages that you md5sum all the files
in the image, then using ddrescue in fill mode you write some data (that
isn't all zero's) to the sections that couldn't be recovered, and then
md5sum all the files again and compare. Seeing as I had already copied
all the files off the image, it was actually even simplier than that. I
wrote the random data to the damaged sections (in this case "BADSECTOR"
over and over again) and then did a diff between the files on the image
and the files I had already copied off the image. It did take awhile to
do the diff, but 4hrs to compare 180Gb of files with 180Gb of files,
over a network isn't that bad. I'm sure it would have been a lot quicker
had all the files resided on the local machine on  a nice RAID array.&lt;/p&gt;
&lt;p&gt;So a simplified example&lt;/p&gt;
&lt;p&gt;copy all files off rescued image (loop mount) to another location&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;BAD SECTOR &amp;quot;&lt;/span&gt; &amp;gt; tmpfile
&lt;span class="nv"&gt;$ &lt;/span&gt;ddrescue --fill&lt;span class="o"&gt;=&lt;/span&gt;- tmpfile rescue.img rescue.log

&lt;span class="nv"&gt;$ &lt;/span&gt;diff -r /mnt/loop/ /mnt/server/rescuedfiles/ &amp;gt; damagedfiles
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You'll then have a list of files that differ between the 2 versions,
which are the ones with damaged sectors. Also, the ddrescue doesn't
damage the logfile so you can then reverse it using /dev/zero to restore
the image to it's original recovery state. This won't work with sparse
files.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;ddrescue --fill&lt;span class="o"&gt;=&lt;/span&gt;- /dev/zero rescue.img rescue.log
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;ins datetime="2011-08-11T21:54:00+00:00"&gt;Or, if you haven't copied the
files off, then the way suggested in the ddrescue manual looks like
this. (After getting your disk image)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# mount -o loop rescued.img /mnt/loop

$ find /mnt/loop -type f -print0 | xargs -0 md5sum &amp;gt; prefill.md5

$ echo -n &amp;quot;BAD SECTOR &amp;quot; &amp;gt; tmpfile
$ ddrescue --fill=- tmpfile rescue.img rescue.log

## You may need to unmount and remount the loop file to prevent any caching interferring.
# umount /mnt/loop
# mount -o loop rescued.img /mnt/loop

$ find /mnt/loop -type f -print0 | xargs -0 md5sum &amp;gt; postfill.md5

$ diff prefill.md5 postfill.md5
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;/ins&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins datetime="2011-08-11T21:42:04+00:00"&gt;&lt;strong&gt;This is all you need when
trying to work out which files are damaged. The following is another
method for really peaking into the file system that may be more useful
for deeper analysis&lt;/strong&gt;&lt;/ins&gt;&lt;/p&gt;
&lt;p&gt;However, if you do want to find out which files are in the damaged
sectors, then continue as it is possible.&lt;/p&gt;
&lt;p&gt;First, check that the sector is actually used. This used to be the dstat
command, but it has since been renamed to blkstat. So we take a sector
number from the logfile that couldn't be recovered.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;cat rescue.log &lt;span class="p"&gt;|&lt;/span&gt;grep -&lt;span class="p"&gt;|&lt;/span&gt;head
0x0178F200  0x0000D400  -
0x017A0000  0x00000200  -
0x2BC488F400  0x00011600  -
0x2BEEFF0A00  0x00020000  -
0x5AC5FB0A00  0x00020000  -
0x5AC6050A00  0x00020000  -
0x5AC60E0A00  0x00020000  -
0x5AC6180A00  0x00020000  -
0x5AC6220A00  0x00020000  -
0x5AC62B0A00  0x00020000  -
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We then convert it to a sector number. I know the sector size is 512
bytes in this case, but you will need to verify it for your drive. If in
doubt, do the conversion, open a hexeditor, jump to that location, use
the previous ddrescue command to fill in the badsectors with some known
next, and confirm that the known text is at the address you are
viewing.&lt;br /&gt;
I've picked sector 0x2BEEFF0A00 to analyise. So I convert it to decimal
and dived by 512 (the sector size). I can do this all at once if I know
that 512 in hex is 200.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ibase=16; 2BEEFF0A00/200&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;bc
368541573
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Or do it the long way&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ibase=16; 2BEEFF0A00&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;bc
188693285376
&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;188693285376/512&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;bc
368541573
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ether way, I now know that the sector number is 368541573. Using blkstat
(formerly dstat) I can verify that the sector is used or not.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;blkstat rescued.img 368541573
Sector: 368541573
Allocated
Cluster: 5754738
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now a little warning, if you get a sector that looks like this.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;blkstat rescued.img 48249
Sector: 48249
Allocated &lt;span class="o"&gt;(&lt;/span&gt;Meta&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then you are probably looking at a sector in a directory listing. The
next tool we are going to use, ifind, may take a very long time to
process that sector for almost no gain. I would leave these sectors
until you have processed the others. I believe this one for me actually
is in one of the FAT's, which just told me that the FAT was probably
damaged. What I can do is work with the damaged FAT initially, then
switch over to the other FAT and see what is different. (Which I won't
cover in this post) &lt;ins datetime="2011-04-18T23:52:56+00:00"&gt;On further
investigation, it turns out this sector was in an unused part of the
FAT, so had caused no damage to the FAT. Damage to the fat could prevent
you from even seeing some of the files, so hopefully when you have
damage to the FAT your 2nd FAT will still be good.&lt;/ins&gt;&lt;/p&gt;
&lt;p&gt;Our next step is to use ifind to find the inode associated with the
sector. This can take some time and lots of CPU.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;time &lt;/span&gt;ifind rescued.img -d 368541573
5892375559

real    2m52.314s
user    1m15.280s
sys 0m2.170s
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This finally gives us the inode associated with that file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;istat rescued.img 5892375559&lt;span class="p"&gt;|&lt;/span&gt;head -12
Directory Entry: 5892375559
Allocated
File Attributes: File, Archive
Size: 21346140
Name: MICROS~1

Directory Entry Times:
Written:    Wed Aug &lt;span class="m"&gt;25&lt;/span&gt; 22:16:38 2010
Accessed:   Wed Apr &lt;span class="m"&gt;13&lt;/span&gt; 00:00:00 2011
Created:    Wed Aug &lt;span class="m"&gt;25&lt;/span&gt; 22:16:39 2010

Sectors:
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The listing then goes on to show all sectors that the file uses. If we
grep the listing, we can confirm that the sector 5892375559 is in that
file. Unfortunately, we are stuck with the 8.3 filename and not the
complete filename. Thankfully another tool will come to our rescue. If
you are looking for a good number of files that are affected (i.e. more
than 1 or 2 sectors) you'll want to run this command without the grep,
and save the contents to a file so you can just grep over that file each
time as it takes a long time to recursively list all the files in a big
filesystem.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;fls -rFp bunsom.img &lt;span class="p"&gt;|&lt;/span&gt;grep 5892375559
r/r 5892375559: Microsoft Office 2011/Office/Microsoft Chart Converter.app/Contents/MacOS/Microsoft Chart Converter
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So finally we can see that the inode 5892375559, which contains our
damaged sector, belongs to this file from the Microsoft Office Chart
Converter App. (Yes, it does look a little strange this file but that's
because it's a OS X App file).&lt;/p&gt;
&lt;p&gt;We can now repeat this for all damaged sectors. However, I'd be first
getting a list of all sectors (remember that the second column in the
ddrescue log is size of the damaged area, use that to work out who many
sectors in a row are damaged). I'd then check the file we have just
found (use the istat tool) to see if any of the other damaged sectors
are also in that file.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;keywords to assist others finding it: data recovery, damaged sector,
file at sector, file belong to sector, sector contents, damaged files,
fat32, sector explore, sector view&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Comments&lt;/h2&gt;
&lt;p&gt;Leave a comment by adding to the &lt;a href="https://github.com/timwhite/technicallytim/issues/1"&gt;issue on
Github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comment from &lt;a href="https://github.com/amichair"&gt;amichair&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thanks for the article, it's been very helpful!&lt;/p&gt;
&lt;p&gt;A few updates that might save time for the next person to need it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;instead of the hex calculations and using bc, one can simply use 'ddrescuelog
-l- ddrescue.log'. This outputs all the bad ('-') sector numbers in decimal,
i.e. it does both the division by sector size and conversion to decimal, and in
addition it outputs all the sectors and not just the first one of each series
of bad blocks (you mention this in the last paragraph but with no example of
how to do this). So this one command takes care of a lot of stuff in one fell
swoop.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in some larger disks nowadays the sector size is 4096 rather than 512. Thus
calculating the 'data unit' used in the sleuthkit tools might require an
additional division by 8 (after division by 512).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the version of sleuthkit in Ubuntu (up to 13.10, and it looks like it won't
change by the 14.04 release this week) is a couple years old, and as it turns
out, does not support ext4. While this is platform-specific (though pretty
common platform), the important note is that blkstat works ok, but then ifind
says 'inode not found', which is a bit perplexing considering it is marked as
'Allocated'. So the filesystem auto-detection fails silently. Only if one
specifies '-f ext4' explicitly does he find out that ext4 is not supported. I
had to download and build the latest sleuthkit myself for it to work with ext4
properly.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks once again for this great tutorial!&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="damaged"></category><category term="fat32"></category><category term="recovery"></category><category term="sector"></category></entry><entry><title>Are mobile Providers Ripping Us Off?</title><link href="http://tim.purewhite.id.au/2011/04/are-mobile-providers-ripping-us-off/" rel="alternate"></link><updated>2011-04-08T18:17:00+10:00</updated><author><name>Tim White</name></author><id>tag:tim.purewhite.id.au,2011-04-08:2011/04/are-mobile-providers-ripping-us-off/</id><summary type="html">&lt;p&gt;With so many options, so many plans, it's hard to know if we are being
ripped off or getting a good deal. Then bring in VoIP, and things get
even trickier.&lt;/p&gt;
&lt;p&gt;For the sake of this research, I was looking for a mobile phone plan,
prepaid or post paid, that would ideally cost less than $40 a month and
didn't rely on any VoIP. (Due to data connections being unreliable and
sometimes VoIP applications are difficult, like calling an access number
then dialing the rest of your number). I firstly crunched just the
numbers, to get an idea of what Mobile Cap plans actually give you in
real dollars, not cap dollars, and how that would translate into minutes
of talk time or sms's. Finally, I took my actual usage logs (which my
Android saves to my Gmail account for me via SMS Backup+), and crunched
the numbers to work out how much that usage would cost me on each plan.&lt;/p&gt;
&lt;p&gt;First, a bit about  my method in crunching the numbers. Most mobile
providers charge in 1 or 3 ways, per second, per 30 seconds, or per
minute. If you make long calls, then per minute billing won't really
make much of a difference, but if you make short calls, then per minute
billing could end up costing you lots. For example, if all my calls are
less than 30 seconds long, and I'm on per minute billing, then I'm
always paying for a minute, even though I've used less than half of
that!&lt;br /&gt;
Using that, I figured that splitting my calls in to 3 categories would
allow me to best see the cost difference between per second, per 30
second, and per minute billing. Calls less than 30 seconds, Calls more
than 30 seconds but less than a minute, and calls longer than a minute.
I then wrote my formulas using the split categories, using the average
call length for each category and the number of calls. To verify that
this wasn't skewing my data, I then took 1 month and calculated the
total cost processing each call to the second, for per second, per 30
second and per minute billing. The results were almost 100% the same
between the 2 methods. You are welcome to show me some mathematical
proof as to why this happens, just know that if I take the average
length of all calls for the month, and don't split them into categories,
then the results are not close and the providers offering per second
billing loose their advantage.&lt;/p&gt;
&lt;p&gt;So, as you may have already worked out, per second billing can really
save you money. Which is probably why more and more providers are moving
away from per second billing towards per minute billing. You'll notice a
lot of VoIP providers advertise per second billing, which is because
people know that they are only paying for what they use. For example, my
scenario outcome for one month on one particular per minute biller,
comes out to $311, but if they offered per 30 second billing like they
previously did, I'd save $32 worth of cap credit, or about $2 of real
money. Not much, but as you can see, it's extra money in the providers
pockets.&lt;/p&gt;
&lt;p&gt;When I started my research, I expected to find that these Cap plans are
ripping us off with the illusion of lots of money (cap credit) for very
little real money. I also expected to find that per second billing would
save us the most money. And I hoped to find a provider that would be
better value than my current one. Well, a lot of my expectations have
been crushed.&lt;/p&gt;
&lt;p&gt;Instead, what I've found is that while the Cap plans do have a very high
call rate, and high flag fall, they are actually good value.&lt;strong&gt;Note
however, that the numbers I'm about to quote assume that you ether use
&lt;em&gt;all&lt;/em&gt; the cap credit on a call, or on sms's, and don't take into account
flagfalls (which would be charged for each call).&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A plan with a $0.99 per minute call rate, costs $0.06 per minute in
real dollars.&lt;/em&gt;&lt;br /&gt;
&lt;em&gt;The same plan with $0.28 sms rate, costs $0.02 per sms in real
dollars.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When I first saw these numbers, I was shocked. So for comparison, I
looked at one of the cheapest VoIP providers. $0.105 per minute, no
flagfall and per second billing. And $0.05 per sms. Yes, you read that
correctly, the mobile phone is cheaper! Now before you trash your VoIP,
that VoIP rate is to mobile phones in Australia, calls to Landlines and
of course, overseas calls will be cheaper.&lt;/p&gt;
&lt;p&gt;One of the plans I was looking at thinking should work out cheaper, is
actually a VoIP providers plan. They provider a mobile sim card, with
decent call rates, and free calls to their VoIP numbers, so you can then
make your normal call. But as you can see above, even with the VoIP call
being made over the mobile network, it is still cheaper with this Cap
plan! I had also been thinking of using a callback VoIP provider, I call
an access number, it hangs up and calls me back, then I dial the number
I want and it calls the other side. No cost on the mobile side (except I
need to have credit to make the call, even though I'm not charged for
the call). Yet this requires 2 calls via the VoIP provider, one back to
my mobile, and one to whom ever I'm calling, and at $0.105 a minute,
that's $0.21 a minute all up!&lt;/p&gt;
&lt;p&gt;So when is VoIP cheaper then? It's cheaper for international, and
untimed calls, for example, calls to landlines in Australia and a number
of other countries.&lt;/p&gt;
&lt;p&gt;So, at the end of the day, Mobile providers aren't ripping us off, well,
not as much as I thought. Yes, per minute billing means more money to
the providers, and Caps that you don't use completely are also wasting
some money. But even with the months of very light usage, the Cap still
works out cheaper than going with other plans.&lt;/p&gt;
&lt;p&gt;My recommendation, go for a Cap plan that gives you enough Cap $'s that
you won't go over (especially if going post-paid) but you also won't
have heaps unused each month. Then, if your phone can, setup VoIP and
use it for calls to fixed lines. (This could be via mobile data or WiFi
depending on how good the 3G is where you live).  If you don't call
mobiles much, or SMS, and get good 3G in your area, then a Data only
plan with VoIP might be a better choice, as long as you are sure you
don't need normal mobile voice service.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://tim.purewhite.id.au/images/2011/04/Phone-Comparison.png"&gt;&lt;img alt="Phone Comparison" src="http://tim.purewhite.id.au/images/2011/04/Phone-Comparison-300x126.png" /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;NB: Yes, I have taken into account Free calls between phones on the same
carriers, however I only used calls between me and my wife as fitting
into this category, as they are the only calls we can guarantee to be on
the same network.&lt;/p&gt;</summary><category term="Cap Plan"></category><category term="Comparison"></category><category term="Exetel"></category><category term="iinet"></category><category term="Mobile"></category><category term="Optus"></category><category term="Pennytel"></category><category term="Phone"></category><category term="Post Paid"></category><category term="Pre Paid"></category><category term="Provider"></category><category term="Telstra"></category><category term="Virgin"></category><category term="voip"></category></entry></feed>